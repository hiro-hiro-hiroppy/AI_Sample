{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# パッケージのインポート\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "# import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20e384bf6b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# DEVICE = 'cuda:0'\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "# 再開モードの場合も, このエポック数の分だけ追加学習される（N_EPOCHSは最終エポック番号ではない）\n",
    "N_EPOCHS = 20\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 訓練データセット（画像ファイルリスト）のファイル名\n",
    "DATASET_CSV = './MNIST/train_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列（データセットが存在するディレクトリのパス）\n",
    "DATA_DIR = './MNIST/'\n",
    "\n",
    "# 特徴ベクトルの次元数\n",
    "N = 32\n",
    "\n",
    "# バッチ正規化を使用するか否か\n",
    "USE_BATCH_NORM = True\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './cgan_models_MNIST/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE_ENC = os.path.join(MODEL_DIR, 'mnist_encoder_model.pth') # エンコーダ\n",
    "MODEL_FILE_DEC = os.path.join(MODEL_DIR, 'mnist_decoder_model.pth') # デコーダ\n",
    "\n",
    "# 中断／再開の際に用いる一時ファイルの保存先\n",
    "CHECKPOINT_EPOCH = os.path.join(MODEL_DIR, 'checkpoint_epoch.pkl')\n",
    "CHECKPOINT_ENC_MODEL = os.path.join(MODEL_DIR, 'checkpoint_enc_model.pth')\n",
    "CHECKPOINT_DEC_MODEL = os.path.join(MODEL_DIR, 'checkpoint_dec_model.pth')\n",
    "CHECKPOINT_ENC_OPT = os.path.join(MODEL_DIR, 'checkpoint_enc_opt.pth')\n",
    "CHECKPOINT_DEC_OPT = os.path.join(MODEL_DIR, 'checkpoint_dec_opt.pth')\n",
    "\n",
    "\n",
    "# 設定\n",
    "workers = 2\n",
    "batch_size=50\n",
    "nz = 100\n",
    "nch_g = 128\n",
    "nch_d = 128\n",
    "n_epoch = 10\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "outf = './result_3_3-CGAN'\n",
    "display_interval = 600\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 保存先ディレクトリを作成\n",
    "try:\n",
    "    os.makedirs(outf, exist_ok=True)\n",
    "except OSError as error: \n",
    "    print(error)\n",
    "    pass\n",
    "\n",
    "# 乱数のシード（種）を固定\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "from mylib.utility import save_datasets, load_datasets_from_file\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
    "if RESTART_MODE:\n",
    "    train_dataset, valid_dataset = load_datasets_from_file(MODEL_DIR)\n",
    "    if train_dataset is None:\n",
    "        print('error: there is no checkpoint previously saved.')\n",
    "        exit()\n",
    "    train_size = len(train_dataset)\n",
    "    valid_size = len(valid_dataset)\n",
    "\n",
    "# そうでない場合は，データセットを読み込む\n",
    "else:\n",
    "\n",
    "    # CSVファイルを読み込み, 訓練データセットを用意\n",
    "    dataset = CSVBasedDataset(\n",
    "        filename = DATASET_CSV,\n",
    "        items = [\n",
    "            'File Path', # X\n",
    "        ],\n",
    "        dtypes = [\n",
    "            'image', # Xの型\n",
    "        ],\n",
    "        dirname = DATA_DIR,\n",
    "    )\n",
    "\n",
    "    # 訓練データセットを分割し，一方を検証用に回す\n",
    "    dataset_size = len(dataset)\n",
    "    valid_size = int(0.01 * dataset_size) # 全体の 1% を検証用に\n",
    "    train_size = dataset_size - valid_size # 残りの 99% を学習用に\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "    # データセット情報をファイルに保存\n",
    "    save_datasets(MODEL_DIR, train_dataset, valid_dataset)\n",
    "\n",
    "# 訓練データおよび検証用データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ネットワークの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    生成器Gのクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, nz=100, nch_g=64, nch=1):\n",
    "        \"\"\"\n",
    "        :param nz: 入力ベクトルzの次元\n",
    "        :param nch_g: 最終層の入力チャネル数\n",
    "        :param nch: 出力画像のチャネル数\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # ニューラルネットワークの構造を定義する\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'layer0': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nz, nch_g * 4, 3, 1, 0),     # 転置畳み込み\n",
    "                nn.BatchNorm2d(nch_g * 4),                      # バッチノーマライゼーション\n",
    "                nn.ReLU()                                       # ReLU\n",
    "            ),  # (B, nz, 1, 1) -> (B, nch_g*4, 3, 3)\n",
    "            'layer1': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g * 4, nch_g * 2, 3, 2, 0),\n",
    "                nn.BatchNorm2d(nch_g * 2),\n",
    "                nn.ReLU()\n",
    "            ),  # (B, nch_g*4, 3, 3) -> (B, nch_g*2, 7, 7)\n",
    "            'layer2': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g * 2, nch_g, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nch_g),\n",
    "                nn.ReLU()\n",
    "            ),  # (B, nch_g*2, 7, 7) -> (B, nch_g, 14, 14)\n",
    "            'layer3': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g, nch, 4, 2, 1),\n",
    "                nn.Tanh()\n",
    "            )   # (B, nch_g, 14, 14) -> (B, nch, 28, 28)\n",
    "        })\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        順方向の演算\n",
    "        :param z: 入力ベクトル\n",
    "        :return: 生成画像\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():  # self.layersの各層で演算を行う\n",
    "            z = layer(z)\n",
    "        return z\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    識別器Dのクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, nch=1, nch_d=64):\n",
    "        \"\"\"\n",
    "        :param nch: 入力画像のチャネル数\n",
    "        :param nch_d: 先頭層の出力チャネル数\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # ニューラルネットワークの構造を定義する\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'layer0': nn.Sequential(\n",
    "                nn.Conv2d(nch, nch_d, 4, 2, 1),     # 畳み込み\n",
    "                nn.LeakyReLU(negative_slope=0.2)    # leaky ReLU関数\n",
    "            ),  # (B, nch, 28, 28) -> (B, nch_d, 14, 14)\n",
    "            'layer1': nn.Sequential(\n",
    "                nn.Conv2d(nch_d, nch_d * 2, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nch_d * 2),\n",
    "                nn.LeakyReLU(negative_slope=0.2)\n",
    "            ),  # (B, nch_d, 14, 14) -> (B, nch_d*2, 7, 7)\n",
    "            'layer2': nn.Sequential(\n",
    "                nn.Conv2d(nch_d * 2, nch_d * 4, 3, 2, 0),\n",
    "                nn.BatchNorm2d(nch_d * 4),\n",
    "                nn.LeakyReLU(negative_slope=0.2)\n",
    "            ),  # (B, nch_d*2, 7, 7) -> (B, nch_d*4, 3, 3)\n",
    "            'layer3': nn.Sequential(\n",
    "                nn.Conv2d(nch_d * 4, 1, 3, 1, 0),\n",
    "                nn.Sigmoid()\n",
    "            )    \n",
    "            # (B, nch_d*4, 3, 3) -> (B, 1, 1, 1)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        順方向の演算\n",
    "        :param x: 本物画像あるいは生成画像\n",
    "        :return: 識別信号\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():  # self.layersの各層で演算を行う\n",
    "            x = layer(x)\n",
    "        return x.squeeze()     # Tensorの形状を(B)に変更して戻り値とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    ニューラルネットワークの重みを初期化する。作成したインスタンスに対しapplyメソッドで適用する\n",
    "    :param m: ニューラルネットワークを構成する層\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:            # 畳み込み層の場合\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:        # 全結合層の場合\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:     # バッチノーマライゼーションの場合\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): ConvTranspose2d(110, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 生成器G。ランダムベクトルから生成画像を作成する\n",
    "netG = Generator(nz=nz+10, nch_g=nch_g).to(device) #10はn_class=10を指す。出し分けに必要なラベル情報\n",
    "netG.apply(weights_init)    # weights_init関数で初期化\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): ConvTranspose2d(110, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 生成器G。ランダムベクトルから生成画像を作成する\n",
    "netG = Generator(nz=nz+10, nch_g=nch_g).to(device) #10はn_class=10を指す。出し分けに必要なラベル情報\n",
    "netG.apply(weights_init)    # weights_init関数で初期化\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): Conv2d(11, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 識別器D。画像が本物か生成かを識別する\n",
    "netD = Discriminator(nch=1+10, nch_d=nch_d).to(device) #10はn_class=10を指す。分類に必要なラベル情報\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()    # バイナリークロスエントロピー（Sigmoid関数無し）\n",
    "\n",
    "# オプティマイザ−のセットアップ\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 識別器D用\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 生成器G用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(label, device, n_class=10):\n",
    "    \"\"\"\n",
    "    カテゴリカル変数のラベルをOne-Hoe形式に変換する\n",
    "    :param label: 変換対象のラベル\n",
    "    :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
    "    :param n_class: ラベルのクラス数\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    eye = torch.eye(n_class, device=device)\n",
    "    # ランダムベクトルあるいは画像と連結するために(B, c_class, 1, 1)のTensorにして戻す\n",
    "    return eye[label].view(-1, n_class, 1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_image_label(image, label, device, n_class=10):\n",
    "    \"\"\"\n",
    "    画像とラベルを連結する\n",
    "    :param image:　画像\n",
    "    :param label: ラベル\n",
    "    :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
    "    :param n_class: ラベルのクラス数\n",
    "    :return:　画像とラベルをチャネル方向に連結したTensor\n",
    "    \"\"\"\n",
    "    B, C, H, W = image.shape    # 画像Tensorの大きさを取得\n",
    "    \n",
    "    oh_label = onehot_encode(label, device)         # ラベルをOne-Hotベクトル化\n",
    "    oh_label = oh_label.expand(B, n_class, H, W)    # 画像のサイズに合わせるようラベルを拡張する\n",
    "    return torch.cat((image, oh_label), dim=1)      # 画像とラベルをチャネル方向（dim=1）で連結する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_noise_label(noise, label, device):\n",
    "    \"\"\"\n",
    "    ノイズ（ランダムベクトル）とラベルを連結する\n",
    "    :param noise: ノイズ\n",
    "    :param label: ラベル\n",
    "    :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
    "    :return:　ノイズとラベルを連結したTensor\n",
    "    \"\"\"\n",
    "    oh_label = onehot_encode(label, device)     # ラベルをOne-Hotベクトル化\n",
    "    return torch.cat((noise, oh_label), dim=1)  # ノイズとラベルをチャネル方向（dim=1）で連結する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 100, 1, 1])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
      "        4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        8, 9])\n",
      "torch.Size([50, 110, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# 生成器のエポックごとの画像生成に使用する確認用の固定ノイズ\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device) \n",
    "# 確認用のラベル。0〜9のラベルの繰り返し\n",
    "fixed_label = [i for i in range(10)] * (batch_size // 10)\n",
    "fixed_label = torch.tensor(fixed_label, dtype=torch.long, device=device)\n",
    "# 確認用のノイズとラベルを連結\n",
    "fixed_noise_label = concat_noise_label(fixed_noise, fixed_label, device) \n",
    "print(fixed_noise.shape)\n",
    "print(fixed_label)\n",
    "print(fixed_noise_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習のループ\n",
    "for epoch in range(n_epoch):\n",
    "    for itr, data in enumerate(dataloader):\n",
    "        real_image = data[0].to(device)     # 本物画像\n",
    "        real_label = data[1].to(device)     # 本物画像に対応するラベル\n",
    "        # 本物画像とラベルを連結\n",
    "        real_image_label = concat_image_label(real_image, real_label, device) \n",
    "        sample_size = real_image.size(0)    # 画像枚数\n",
    "\n",
    "        # 標準正規分布からノイズを生成\n",
    "        noise = torch.randn(sample_size, nz, 1, 1, device=device)\n",
    "        # 生成画像生成用のラベル\n",
    "        fake_label = torch.randint(10, (sample_size,), dtype=torch.long, device=device)\n",
    "        # ノイズとラベルを連結\n",
    "        fake_noise_label = concat_noise_label(noise, fake_label, device)        \n",
    "        # 本物画像に対する識別信号の目標値「1」\n",
    "        real_target = torch.full((sample_size,), 1., device=device)\n",
    "        # 生成画像に対する識別信号の目標値「0」\n",
    "        fake_target = torch.full((sample_size,), 0., device=device)\n",
    "        \n",
    "        ############################\n",
    "        # 識別器Dの更新\n",
    "        ###########################\n",
    "        netD.zero_grad()    # 勾配の初期化\n",
    "        \n",
    "        # 識別器Dで本物画像とラベルの組み合わせに対する識別信号を出力\n",
    "        output = netD(real_image_label)\n",
    "        # 本物画像に対する識別信号の損失値\n",
    "        errD_real = criterion(output, real_target)\n",
    "\n",
    "        D_x = output.mean().item()  # 本物画像の識別信号の平均\n",
    "\n",
    "        fake_image = netG(fake_noise_label)  # 生成器Gでラベルに対応した生成画像を生成\n",
    "        # 生成画像とラベルを連結\n",
    "        fake_image_label = concat_image_label(fake_image, fake_label, device)   \n",
    "        \n",
    "        # 識別器Dで本物画像に対する識別信号を出力\n",
    "        output = netD(fake_image_label.detach()) \n",
    "        # 生成画像に対する識別信号の損失値\n",
    "        errD_fake = criterion(output, fake_target)  \n",
    "        D_G_z1 = output.mean().item()# 生成画像の識別信号の平均\n",
    "\n",
    "        errD = errD_real + errD_fake    # 識別器Dの全体の損失\n",
    "        errD.backward()    # 誤差逆伝播\n",
    "        optimizerD.step()   # Dのパラメーターを更新\n",
    "\n",
    "        ############################\n",
    "        # 生成器Gの更新\n",
    "        ###########################\n",
    "        netG.zero_grad()    # 勾配の初期化\n",
    "        \n",
    "        output = netD(fake_image_label)     # 更新した識別器Dで改めて生成画像とラベルの組み合わせに対する識別信号を出力\n",
    "        errG = criterion(output, real_target)   # 生成器Gの損失値。Dに生成画像を本物画像と誤認させたいため目標値は「1」\n",
    "        errG.backward()     # 誤差逆伝播\n",
    "        D_G_z2 = output.mean().item()# 更新した識別器Dによる生成画像の識別信号の平均\n",
    "\n",
    "        optimizerG.step()   # Gのパラメータを更新\n",
    "\n",
    "        if itr % display_interval == 0: \n",
    "            print('[{}/{}][{}/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}/{:.3f}'\n",
    "                  .format(epoch + 1, n_epoch,\n",
    "                          itr + 1, len(dataloader),\n",
    "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        if epoch == 0 and itr == 0:     # 初回に本物画像を保存する\n",
    "            vutils.save_image(real_image, '{}/real_samples.png'.format(outf),\n",
    "                              normalize=True, nrow=10)\n",
    "\n",
    "    ############################\n",
    "    # 確認用画像の生成\n",
    "    ############################\n",
    "    fake_image = netG(fixed_noise_label)  # 1エポック終了ごとに確認用の生成画像を生成する\n",
    "    vutils.save_image(fake_image.detach(), '{}/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1),\n",
    "                      normalize=True, nrow=10)\n",
    "\n",
    "    ############################\n",
    "    # モデルの保存\n",
    "    ############################\n",
    "    if (epoch + 1) % 10 == 0:   # 10エポックごとにモデルを保存する\n",
    "        torch.save(netG.state_dict(), '{}/netG_epoch_{}.pth'.format(outf, epoch + 1))\n",
    "        torch.save(netD.state_dict(), '{}/netD_epoch_{}.pth'.format(outf, epoch + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
